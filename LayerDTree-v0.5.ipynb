{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本文主要以kaggle一次二分类竞赛数据为例，深入研究改进forest\n",
    "\n",
    "LayerDTree是LayerForest的精简版，是对Forest进一步探索的基础\n",
    "\n",
    "- 数据地址：https://www.kaggle.com/c/porto-seguro-safe-driver-prediction\n",
    "- 数据特点：\n",
    "  - 非常不均衡、缺失值严重、二分类\n",
    "- 包含内容：\n",
    "  1. 数据获取\n",
    "  2. 模型应用\n",
    "  3. 结果分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage-1：获取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import os.path as osp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FeatureParser(object):\n",
    "    def __init__(self, desc):\n",
    "        desc = desc.strip()\n",
    "        if desc == \"C\":\n",
    "            self.f_type = \"number\"\n",
    "        else:\n",
    "            self.f_type = \"categorical\"\n",
    "            f_names = [d.strip() for d in desc.split(\",\")]\n",
    "            # missing value\n",
    "            f_names.insert(0, \"?\")\n",
    "            self.name2id = dict(zip(f_names, range(len(f_names))))\n",
    "\n",
    "    def get_float(self, f_data):\n",
    "        f_data = f_data.strip()\n",
    "        if self.f_type == \"number\":\n",
    "            return float(f_data)\n",
    "        return float(self.name2id[f_data])\n",
    "\n",
    "    def get_data(self, f_data):\n",
    "        f_data = f_data.strip()\n",
    "        if self.f_type == \"number\":\n",
    "            return float(f_data)\n",
    "        data = np.zeros(len(self.name2id), dtype=np.float32)\n",
    "        data[self.name2id[f_data]] = 1\n",
    "        return data\n",
    "\n",
    "    def get_fdim(self):\n",
    "        \"\"\"\n",
    "        get feature dimension\n",
    "        \"\"\"\n",
    "        if self.f_type == \"number\":\n",
    "            return 1\n",
    "        return len(self.name2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('.\\\\datasetes\\\\adult\\\\adult.data',\n",
       " '.\\\\datasetes\\\\adult\\\\adult.test',\n",
       " '.\\\\datasetes\\\\adult\\\\features')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_path = osp.join(\".\\\\datasetes\", \"adult\", \"adult.data\")\n",
    "test_data_path = osp.join(\".\\\\datasetes\", \"adult\", \"adult.test\")\n",
    "feature_desc_path = osp.join(\".\\\\datasetes\", \"adult\", \"features\")\n",
    "train_data_path, test_data_path, feature_desc_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f_parsers = []\n",
    "with open(feature_desc_path) as f:\n",
    "    for row in f.readlines():\n",
    "        f_parsers.append(FeatureParser(row))\n",
    "# f_parsers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32561, 14) (32561,)\n"
     ]
    }
   ],
   "source": [
    "with open(train_data_path) as f:\n",
    "    rows = [row.strip().split(\",\") for row in f.readlines() if len(row.strip()) > 0 and not row.startswith(\"|\")]\n",
    "n_datas = len(rows)\n",
    "\n",
    "cate_as_onehot = 0\n",
    "if cate_as_onehot:\n",
    "    X_dim = np.sum([f_parser.get_fdim() for f_parser in f_parsers])\n",
    "    X = np.zeros((n_datas, X_dim), dtype=np.float32)\n",
    "else:\n",
    "    X = np.zeros((n_datas, 14), dtype=np.float32)\n",
    "y = np.zeros(n_datas, dtype=np.int32)\n",
    "for i, row in enumerate(rows):\n",
    "    assert len(row) == 15, \"len(row) wrong, i={}\".format(i)\n",
    "    foffset = 0\n",
    "    for j in range(14):\n",
    "        if cate_as_onehot:\n",
    "            fdim = f_parsers[j].get_fdim()\n",
    "            X[i, foffset:foffset+fdim] = f_parsers[j].get_data(row[j].strip())\n",
    "            foffset += fdim\n",
    "        else:\n",
    "            X[i, j] = f_parsers[j].get_float(row[j].strip())\n",
    "    y[i] = 0 if row[-1].strip().startswith(\"<=50K\") else 1\n",
    "print(X.shape, y.shape)\n",
    "X_train = X\n",
    "y_train = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16281, 14) (16281,)\n"
     ]
    }
   ],
   "source": [
    "with open(test_data_path) as f:\n",
    "    rows = [row.strip().split(\",\") for row in f.readlines() if len(row.strip()) > 0 and not row.startswith(\"|\")]\n",
    "n_datas = len(rows)\n",
    "\n",
    "cate_as_onehot = 0\n",
    "if cate_as_onehot:\n",
    "    X_dim = np.sum([f_parser.get_fdim() for f_parser in f_parsers])\n",
    "    X = np.zeros((n_datas, X_dim), dtype=np.float32)\n",
    "else:\n",
    "    X = np.zeros((n_datas, 14), dtype=np.float32)\n",
    "y = np.zeros(n_datas, dtype=np.int32)\n",
    "for i, row in enumerate(rows):\n",
    "    assert len(row) == 15, \"len(row) wrong, i={}\".format(i)\n",
    "    foffset = 0\n",
    "    for j in range(14):\n",
    "        if cate_as_onehot:\n",
    "            fdim = f_parsers[j].get_fdim()\n",
    "            X[i, foffset:foffset+fdim] = f_parsers[j].get_data(row[j].strip())\n",
    "            foffset += fdim\n",
    "        else:\n",
    "            X[i, j] = f_parsers[j].get_float(row[j].strip())\n",
    "    y[i] = 0 if row[-1].strip().startswith(\"<=50K\") else 1\n",
    "print(X.shape, y.shape)\n",
    "X_sub = X\n",
    "y_sub = y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage-2：模型应用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import ForestUtils\n",
    "import time\n",
    "import random\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import EnhancedDTree\n",
    "import EnhancedForest\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'EnhancedForest' from 'C:\\\\github_workspace\\\\LayerForest\\\\EnhancedForest.py'>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(EnhancedForest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 决策树算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = DecisionTreeClassifier()\n",
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data auc 0.74431797296\n"
     ]
    }
   ],
   "source": [
    "p_train = clf.predict_proba(X_sub)\n",
    "p_train = [item[1] for item in p_train]\n",
    "p_train = np.array(p_train)\n",
    "print(\"data auc\", metrics.roc_auc_score(y_sub, p_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data auc 0.813402125177\n"
     ]
    }
   ],
   "source": [
    "p_train = clf.predict(X_sub)\n",
    "print(\"data auc\", metrics.accuracy_score(y_sub, p_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 随机森林算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=10, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=50, n_jobs=4,\n",
       "            oob_score=False, random_state=1024, verbose=True,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators=50, max_depth=10, n_jobs=4, random_state=1024, verbose=True)\n",
    "rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done  50 out of  50 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=10, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=50, n_jobs=4,\n",
       "            oob_score=False, random_state=1024, verbose=True,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data auc 0.913601941746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done  50 out of  50 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "p_train = rf.predict_proba(X_sub)\n",
    "p_train = [item[1] for item in p_train]\n",
    "p_train = np.array(p_train)\n",
    "print(\"data auc\", metrics.roc_auc_score(y_sub, p_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data acc 0.859652355506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done  50 out of  50 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "p_train = rf.predict(X_sub)\n",
    "print(\"data acc\", metrics.accuracy_score(y_sub, p_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGB算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'silent': True,\n",
    "    \n",
    "    'max_depth': 4,\n",
    "    'eta': 0.020,\n",
    "    'gamma': 0.65,\n",
    "    \n",
    "    'colsample_bytree': 0.8,\n",
    "    'subsample': 0.6,\n",
    "    \n",
    "    'num_boost_round' : 700,\n",
    "#     'min_child_weight': 10.0,\n",
    "#     'max_delta_step': 1.8,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the gini metric - from https://www.kaggle.com/c/ClaimPredictionChallenge/discussion/703#5897\n",
    "def ginic(actual, pred):\n",
    "    actual = np.asarray(actual) #In case, someone passes Series or list\n",
    "    n = len(actual)\n",
    "    a_s = actual[np.argsort(pred)]\n",
    "    a_c = a_s.cumsum()\n",
    "    giniSum = a_c.sum() / a_s.sum() - (n + 1) / 2.0\n",
    "    return giniSum / n\n",
    "\n",
    "def gini_normalized(a, p):\n",
    "#     if p.ndim == 2:#Required for sklearn wrapper\n",
    "#         p = p[:,1] #If proba array contains proba for both 0 and 1 classes, just pick class 1\n",
    "    return ginic(a, p) / ginic(a, a)\n",
    "\n",
    "# Create an XGBoost-compatible metric from Gini\n",
    "def gini_xgb(preds, dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    gini_score = gini_normalized(labels, preds)\n",
    "    return 'gini', gini_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-gini:0.691894\tvalid-gini:0.700265\n",
      "Multiple eval metrics have been passed: 'valid-gini' will be used for early stopping.\n",
      "\n",
      "Will train until valid-gini hasn't improved in 100 rounds.\n",
      "[100]\ttrain-gini:0.819327\tvalid-gini:0.816554\n",
      "[200]\ttrain-gini:0.836354\tvalid-gini:0.830442\n",
      "[300]\ttrain-gini:0.847753\tvalid-gini:0.839445\n",
      "[400]\ttrain-gini:0.856274\tvalid-gini:0.845215\n",
      "[500]\ttrain-gini:0.861858\tvalid-gini:0.848523\n",
      "[600]\ttrain-gini:0.866693\tvalid-gini:0.850783\n",
      "[700]\ttrain-gini:0.870262\tvalid-gini:0.851864\n",
      "[800]\ttrain-gini:0.87336\tvalid-gini:0.853076\n",
      "[900]\ttrain-gini:0.876152\tvalid-gini:0.853475\n",
      "[1000]\ttrain-gini:0.878787\tvalid-gini:0.854279\n",
      "[1100]\ttrain-gini:0.881256\tvalid-gini:0.854657\n",
      "[1200]\ttrain-gini:0.883566\tvalid-gini:0.854888\n",
      "[1300]\ttrain-gini:0.885647\tvalid-gini:0.855086\n",
      "[1400]\ttrain-gini:0.887623\tvalid-gini:0.855218\n",
      "[1500]\ttrain-gini:0.88951\tvalid-gini:0.85506\n",
      "Stopping. Best iteration:\n",
      "[1412]\ttrain-gini:0.88786\tvalid-gini:0.855257\n",
      "\n"
     ]
    }
   ],
   "source": [
    "d_train = xgb.DMatrix(X_train, y_train)\n",
    "d_valid = xgb.DMatrix(X_sub, y_sub)\n",
    "watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n",
    "\n",
    "mdl = xgb.train(params, d_train, \n",
    "                    num_boost_round=1600, evals=watchlist, early_stopping_rounds=100, \n",
    "                    feval=gini_xgb, maximize=True, verbose_eval=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data auc 0.927533951378\n"
     ]
    }
   ],
   "source": [
    "d_test = xgb.DMatrix(X_sub)\n",
    "p_train = mdl.predict(d_test)\n",
    "print(\"data auc\", metrics.roc_auc_score(y_sub, p_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.87414777962041645"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y_acc_index = np.where(p_train > 0.5)[0]\n",
    "test_y_acc = np.array([0] * len(p_train))\n",
    "test_y_acc[test_y_acc_index] = 1\n",
    "metrics.accuracy_score(y_sub, test_y_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# layerDTree算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'EnhancedForest' from 'C:\\\\github_workspace\\\\LayerForest\\\\EnhancedForest.py'>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(EnhancedForest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gini(actual, pred, cmpcol = 0, sortcol = 1):\n",
    "    assert( len(actual) == len(pred) )\n",
    "    all = np.asarray(np.c_[ actual, pred, np.arange(len(actual)) ], dtype=np.float)\n",
    "    all = all[ np.lexsort((all[:,2], -1*all[:,1])) ]\n",
    "    totalLosses = all[:,0].sum()\n",
    "    giniSum = all[:,0].cumsum().sum() / totalLosses\n",
    "    \n",
    "    giniSum -= (len(actual) + 1) / 2.\n",
    "    return giniSum / len(actual)\n",
    "\n",
    "def gini_normalized(a, p):\n",
    "    return gini(a, p) / gini(a, a)\n",
    "\n",
    "def gini_metrix(a, p):\n",
    "    return \"nor gini:\", gini_normalized(a,p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[812/14870] 1 [1261/15020] [912/14586] 2 [656/14364] [400/14886] 3 [224/14140] [327/14901] 4 [145/13995] [320/14818] 5 [131/13864] [322/14624] 6 [90/13774] [283/14587] 7 [100/13674] [147/14673] 8 [64/13610] [107/14711] 9 [39/13571] [235/14547] 10 [64/13507] [75/14625] 11 [2/13505] [147/14551] 12 [82/13423] [153/14439] 13 [56/13367] [139/14437] 14 [44/13323] [215/14309] 15 [115/13208] [110/14406] 16 [42/13166] [100/14412] 17 [41/13125] [77/14431] 18 [34/13091] [63/14399] 19 [14/13077] [103/14351] 20 [64/13013] [61/14391] 21 [15/12998] [200/14242] 22 [121/12877] [38/14392] 23 [15/12862] [74/14348] 24 [24/12838] [83/14327] 25 [10/12828] [87/14323] 26 [40/12788] [34/14310] 27 [17/12771] [99/14235] 28 [38/12733] [66/14174] 29 [16/12717] [24/14200] 30 [21/12696] [42/14174] 31 [16/12680] [22/14182] 32 [7/12673] [85/14117] 33 [60/12613] [17/14179] 34 [5/12608] [12/14164] 35 [4/12604] [37/14129] 36 [13/12591] [25/14131] 37 [12/12579] [109/14037] 38 [35/12544] [26/14056] 39 [10/12534] [29/14051] 40 [10/12524] [50/14026] 41 [23/12501] [157/13917] 42 [79/12422] [63/13959] 43 [37/12385] [56/13948] 44 [30/12355] [42/13902] 45 [9/12346] [23/13921] 46 [6/12340] [47/13897] 47 [14/12326] [26/13906] 48 [6/12320] [14/13908] 49 [6/12314] [2/13920] 50 [2/12312] [8/13912] 51 [3/12309] [51/13865] 52 [22/12287] [55/13861] 53 [30/12257] [17/13897] 54 [15/12242] [53/13861] 55 [34/12208] [9/13903] 56 [6/12202] [15/13893] 57 [4/12198] [35/13873] 58 [11/12187] [14/13892] 59 [12/12175] [34/13868] 60 [16/12159] [3/13899] 61 [5/12154] [19/13883] 62 [9/12145] [35/13859] 63 [9/12136] [32/13844] 64 [6/12130] [17/13859] 65 [10/12120] [24/13842] 66 [19/12101] [38/13826] 67 [10/12091] [38/13824] 68 [18/12073] [3/13855] 69 [3/12070] [6/13852] 70 [7/12063] [28/13830] 71 [7/12056] [1/13847] 72 [1/12055] [6/13842] 73 [1/12054] [22/13824] 74 [2/12052] [22/13822] 75 [9/12043] [13/13829] 76 [7/12036] [17/13817] 77 [1/12035] [4/13828] 78 [0/12035] [3/13827] 79 [0/12035] [22/13808] 80 [2/12033] [5/13825] 81 [2/12031] [51/13773] 82 [3/12028] [11/13813] 83 [2/12026] [16/13806] 84 [4/12022] [25/13797] 85 [17/12005] [2/13820] 86 [0/12005] [19/13801] 87 [9/11996] [48/13770] 88 [19/11977] [12/13798] 89 [7/11970] [2/13808] 90 [2/11968] [2/13806] 91 [0/11968] [22/13786] 92 [24/11944] [11/13795] 93 [2/11942] [11/13791] 94 [0/11942] [7/13793] 95 [3/11939] [13/13787] 96 [11/11928] [3/13795] 97 [3/11925] [14/13784] 98 [0/11925] [12/13786] 99 [4/11921] [46/13742] 100 [4/11917] [12/13776] 101 [6/11911] [53/13735] 102 [37/11874] [8/13770] 103 [2/11872] [14/13764] 104 [6/11866] [4/13770] 105 [1/11865] [23/13751] 106 [7/11858] [14/13760] 107 [3/11855] [17/13753] 108 [12/11843] [1/13765] 109 [1/11842] [16/13750] 110 [2/11840] [8/13758] 111 [3/11837] [2/13762] 112 [0/11837] [3/13761] 113 [1/11836] [1/13763] 114 [0/11836] [7/13757] 115 [3/11833] [26/13738] 116 [13/11820] [15/13697] 117 [6/11814] [5/13707] 118 [3/11811] [7/13705] 119 [2/11809] [6/13706] 120 [2/11807] [6/13706] 121 [4/11803] [3/13709] 122 [1/11802] [32/13680] 123 [6/11796] [5/13653] 124 [2/11794] [11/13647] 125 [8/11786] [0/13658] 126 [0/11786] [2/13656] 127 [0/11786] [21/13637] 128 [4/11782] [3/13653] 129 [0/11782] [5/13651] 130 [3/11779] [6/13646] 131 [0/11779] [3/13641] 132 [1/11778] [2/13640] 133 [2/11776] [2/13640] 134 [0/11776] [11/13631] 135 [12/11764] [29/13613] 136 [12/11752] [14/13624] 137 [4/11748] [2/13632] 138 [0/11748] [26/13608] 139 [10/11738] [12/13614] 140 [7/11731] [23/13599] 141 [6/11725] [0/13620] 142 [0/11725] [8/13612] 143 [0/11725] [7/13613] 144 [1/11724] [4/13616] 145 [0/11724] [9/13611] 146 [4/11720] [1/13619] 147 [0/11720] [6/13614] 148 [3/11717] [9/13609] 149 [2/11715] [0/13616] 150 [0/11715] [7/13609] 151 [7/11708] [4/13612] 152 [3/11705] [5/13609] 153 [5/11700] [28/13586] 154 [2/11698] [16/13598] 155 [15/11683] [12/13602] 156 [6/11677] [6/13606] 157 [0/11677] [1/13611] 158 [0/11677] [1/13611] 159 [0/11677] [30/13580] 160 [25/11652] [3/13607] 161 [1/11651] [22/13588] 162 [5/11646] [0/13610] 163 [0/11646] [1/13609] 164 [0/11646] [1/13609] 165 [0/11646] [4/13606] 166 [2/11644] [1/13609] 167 [0/11644] [1/13607] 168 [2/11642] [3/13605] 169 [1/11641] [2/13606] 170 [0/11641] [2/13604] 171 [0/11641] [7/13599] 172 [3/11638] [2/13604] 173 [1/11637] [19/13587] 174 [8/11629] [3/13595] 175 [4/11625] [2/13594] 176 [0/11625] [6/13590] 177 [2/11623] [4/13590] 178 [0/11623] [0/13594] 179 [0/11623] [17/13577] 180 [6/11617] [2/13592] 181 [0/11617] [2/13592] 182 [1/11616] [3/13591] 183 [2/11614] [6/13588] 184 [2/11612] [0/13594] 185 [0/11612] [5/13589] 186 [1/11611] [3/13589] 187 [2/11609] [30/13562] 188 [9/11600] [20/13566] 189 [18/11582] [1/13585] 190 [1/11581] [0/13586] 191 [0/11581] [1/13585] 192 [0/11581] [0/13584] 193 [0/11581] [5/13579] 194 [4/11577] [2/13582] 195 [0/11577] [7/13577] 196 [6/11571] [2/13580] 197 [0/11571] [1/13581] 198 [1/11570] [15/13567] 199 [3/11567] [6/13576] 200 [1/11566] [0/13582] 201 [0/11566] [7/13575] 202 [7/11559] [1/13579] 203 [1/11558] [4/13576] 204 [4/11554] [0/13580] 205 [0/11554] [0/13580] 206 [1/11553] [2/13578] 207 [0/11553] [13/13567] 208 [1/11552] [7/13571] 209 [1/11551] [4/13574] 210 [0/11551] [5/13571] 211 [1/11550] [5/13571] 212 [0/11550] [0/13576] 213 [0/11550] [5/13571] 214 [3/11547] [12/13564] 215 [2/11545] [0/13572] 216 [0/11545] [16/13556] 217 [2/11543] [0/13570] 218 [0/11543] [2/13568] 219 [0/11543] [8/13562] 220 [2/11541] [5/13565] 221 [0/11541] [4/13566] 222 [0/11541] [1/13569] 223 [1/11540] [0/13570] 224 [0/11540] [3/13567] 225 [0/11540] [0/13564] 226 [1/11539] [3/13561] 227 [0/11539] [2/13562] 228 [0/11539] [1/13563] 229 [0/11539] [12/13552] 230 [6/11533] [0/13564] 231 [0/11533] [6/13558] 232 [1/11532] [25/13539] 233 [15/11517] [8/13518] 234 [4/11513] [14/13498] 235 [9/11504] [7/13505] 236 [3/11501] [10/13502] 237 [3/11498] [2/13492] 238 [0/11498] [0/13494] 239 [0/11498] [0/13494] 240 [1/11497] [0/13494] 241 [0/11497] [10/13484] 242 [3/11494] [3/13491] 243 [1/11493] [0/13494] 244 [2/11491] [12/13482] 245 [5/11486] [7/13485] 246 [4/11482] [4/13488] 247 [2/11480] [1/13491] 248 [1/11479] [1/13491] 249 [0/11479] [2/13490] 250 [0/11479] [2/13490] 251 [1/11478] [20/13472] 252 [13/11465] [9/13483] 253 [0/11465] [6/13486] 254 [2/11463] [16/13476] 255 [12/11451] [11/13467] 256 [2/11449] [5/13473] 257 [0/11449] [32/13446] 258 [24/11425] [10/13464] 259 [6/11419] [0/13464] 260 [0/11419] [2/13462] 261 [1/11418] [1/13463] 262 [0/11418] [3/13461] 263 [0/11418] [1/13463] 264 [0/11418] [1/13463] 265 [1/11417] [26/13438] 266 [13/11404] [0/13462] 267 [0/11404] [9/13453] 268 [2/11402] [4/13454] 269 [2/11400] [1/13451] 270 [0/11400] [0/13452] 271 [1/11399] [0/13452] 272 [0/11399] [0/13452] 273 [2/11397] [0/13452] 274 [0/11397] [17/13435] 275 [13/11384] [3/13447] 276 [1/11383] [4/13446] 277 [2/11381] [1/13449] 278 [2/11379] [2/13448] 279 [0/11379] [0/13448] 280 [0/11379] [6/13442] 281 [4/11375] [30/13408] 282 [18/11357] [4/13434] 283 [0/11357] [21/13417] 284 [9/11348] [2/13436] 285 [2/11346] [9/13429] 286 [5/11341] [2/13436] 287 [0/11341] [12/13426] 288 [7/11334] [10/13426] 289 [0/11334] [2/13434] 290 [0/11334] [6/13430] 291 [9/11325] [9/13427] 292 [4/11321] [0/13436] 293 [0/11321] [2/13434] 294 [0/11321] [0/13436] 295 [0/11321] [1/13435] 296 [2/11319] [5/13431] 297 [1/11318] [0/13436] 298 [0/11318] [1/13435] 299 [1/11317] [0/13436] 300 [0/11317] [2/13434] 301 [0/11317] 11317\n"
     ]
    }
   ],
   "source": [
    "X = X_train.copy()\n",
    "y = y_train.copy()\n",
    "\n",
    "X_test = X_sub.copy()\n",
    "test_y = np.array(([0.0] * len(X_test)))\n",
    "all_data_mask = np.array([False] * len(X_test))\n",
    "real_y = y_sub.copy()\n",
    "\n",
    "# 均衡数据进行layer\n",
    "X_train_np = X\n",
    "y_train_np = y\n",
    "maxlayer = 300\n",
    "layer = 0\n",
    "\n",
    "enhancedForest = EnhancedForest.EnhancedForest()\n",
    "counter = 0\n",
    "while 1:\n",
    "    layer += 1\n",
    "    X = X_train_np\n",
    "    y = y_train_np\n",
    "    \n",
    "    # 均衡数据\n",
    "    positive_mask = np.where(y == 1)[0]\n",
    "    negative_index = np.where(y == 0)[0]\n",
    "    random.shuffle(negative_index)\n",
    "    negative_mask = negative_index[:len(positive_mask)]\n",
    "    train_mask = np.hstack((positive_mask, negative_mask))\n",
    "    train_data_x = X[train_mask]\n",
    "    train_data_y = y[train_mask]\n",
    "    guest_mask = negative_index[len(positive_mask):]\n",
    "    guest_data_x = X[guest_mask]\n",
    "    guest_data_y = y[guest_mask]\n",
    "    \n",
    "#     print(\"train==1\", train_data_y[train_data_y == 1].shape)\n",
    "#     print(\"train==0\", train_data_y[train_data_y == 0].shape)\n",
    "    clf, data_mask, all_false_data_index, p_test  = \\\n",
    "        enhancedForest.TrainModelLayer(train_data_x, train_data_y, X_test, all_data_mask, test_y, real_y, verbose=False, feval=gini_metrix)\n",
    "        \n",
    "    X_train_np = enhancedForest.X_train_np\n",
    "    y_train_np = enhancedForest.y_train_np\n",
    "    \n",
    "    # 均衡数据\n",
    "    X_train_np = np.vstack((X_train_np, guest_data_x))\n",
    "    y_train_np = np.hstack((y_train_np, guest_data_y))\n",
    "#     print(\"train==1\", y_train_np[y_train_np == 1].shape)\n",
    "#     print(\"train==0\", y_train_np[y_train_np == 0].shape)\n",
    "    pass_data_id = data_mask[data_mask==True]\n",
    "    all_false_data_index = np.where(all_data_mask == False)[0]\n",
    "    X_test_np = X_test[all_false_data_index]\n",
    "    print(\"%d [%d/%d] \" % (layer, len(pass_data_id), len(X_test_np) - len(pass_data_id)), end=\"\")\n",
    "    \n",
    "    if X_train_np.shape[0] < 1000 or y_train_np[y_train_np==1].shape[0] <= 10 \\\n",
    "        or layer > maxlayer \\\n",
    "        or len(p_test[~data_mask]) == 0:\n",
    "        all_data_mask = all_false_data_index[~data_mask]\n",
    "        test_y[all_data_mask] = p_test[~data_mask]\n",
    "        print(len(p_test[~data_mask]))\n",
    "        break\n",
    "        \n",
    "    all_data_mask[~all_data_mask] = data_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 均衡数据进行layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.90604666888726215"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.roc_auc_score(y_sub, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.81635034703028064"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y_acc_index = np.where(test_y > 0.5)[0]\n",
    "test_y_acc = np.array([0] * len(test_y))\n",
    "test_y_acc[test_y_acc_index] = 1\n",
    "metrics.accuracy_score(y_sub, test_y_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 不均衡数据进行layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.90945992483848936"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.roc_auc_score(y_sub, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85430870339659726"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y_acc_index = np.where(test_y > 0.5)[0]\n",
    "test_y_acc = np.array([0] * len(test_y))\n",
    "test_y_acc[test_y_acc_index] = 1\n",
    "metrics.accuracy_score(y_sub, test_y_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7144/25417] 1 [3517/12764] [1440/23977] 2 [766/11998] [731/23246] 3 [372/11626] [306/22940] 4 [163/11463] [223/22717] 5 [107/11356] [222/22495] 6 [100/11256] [154/22341] 7 [83/11173] [229/22112] 8 [102/11071] [137/21975] 9 [68/11003] [89/21886] 10 [44/10959] [165/21721] 11 [84/10875] [68/21653] 12 [33/10842] [41/21612] 13 [24/10818] [24/21588] 14 [15/10803] [57/21531] 15 [29/10774] [28/21503] 16 [21/10753] [19/21484] 17 [8/10745] [52/21432] 18 [30/10715] [34/21398] 19 [13/10702] [59/21339] 20 [30/10672] [22/21317] 21 [6/10666] [18/21299] 22 [7/10659] [33/21266] 23 [14/10645] [55/21211] 24 [22/10623] [8/21203] 25 [3/10620] [5/21198] 26 [3/10617] [16/21182] 27 [6/10611] [64/21118] 28 [32/10579] [12/21106] 29 [8/10571] [7/21099] 30 [7/10564] [25/21074] 31 [15/10549] [12/21062] 32 [4/10545] [16/21046] 33 [8/10537] [17/21029] 34 [8/10529] [60/20969] 35 [31/10498] [15/20954] 36 [10/10488] [14/20940] 37 [9/10479] [16/20924] 38 [8/10471] [35/20889] 39 [8/10463] [11/20878] 40 [8/10455] [13/20865] 41 [5/10450] [16/20849] 42 [6/10444] [17/20832] 43 [9/10435] [12/20820] 44 [3/10432] [21/20799] 45 [18/10414] [6/20793] 46 [4/10410] [19/20774] 47 [13/10397] [15/20759] 48 [13/10384] [1/20758] 49 [0/10384] [12/20746] 50 [5/10379] [8/20738] 51 [1/10378] [50/20688] 52 [19/10359] [12/20676] 53 [7/10352] [0/20676] 54 [0/10352] [0/20676] 55 [0/10352] [0/20676] 56 [0/10352] [0/20676] 57 [0/10352] [0/20676] 58 [0/10352] [0/20676] 59 [0/10352] [0/20676] 60 [0/10352] [0/20676] 61 [0/10352] [0/20676] 62 [0/10352] [0/20676] 63 [0/10352] [0/20676] 64 [0/10352] [0/20676] 65 [0/10352] [0/20676] 66 [0/10352] [0/20676] 67 [0/10352] [0/20676] 68 [0/10352] [0/20676] 69 [0/10352] [0/20676] 70 [0/10352] [0/20676] 71 [0/10352] [0/20676] 72 [0/10352] [0/20676] 73 [0/10352] [0/20676] 74 [0/10352] [0/20676] 75 [0/10352] [0/20676] 76 [0/10352] [0/20676] 77 [0/10352] [0/20676] 78 [0/10352] [0/20676] 79 [0/10352] [0/20676] 80 [0/10352] [0/20676] 81 [0/10352] [0/20676] 82 [0/10352] [0/20676] 83 [0/10352] [0/20676] 84 [0/10352] [0/20676] 85 [0/10352] [0/20676] 86 [0/10352] [0/20676] 87 [0/10352] [0/20676] 88 [0/10352] [0/20676] 89 [0/10352] [0/20676] 90 [0/10352] [0/20676] 91 [0/10352] [0/20676] 92 [0/10352] [0/20676] 93 [0/10352] [0/20676] 94 [0/10352] [0/20676] 95 [0/10352] [0/20676] 96 [0/10352] [0/20676] 97 [0/10352] [0/20676] 98 [0/10352] [0/20676] 99 [0/10352] [0/20676] 100 [0/10352] [0/20676] 101 [0/10352] [0/20676] 102 [0/10352] [0/20676] 103 [0/10352] [0/20676] 104 [0/10352] [0/20676] 105 [0/10352] [0/20676] 106 [0/10352] [0/20676] 107 [0/10352] [0/20676] 108 [0/10352] [0/20676] 109 [0/10352] [0/20676] 110 [0/10352] [0/20676] 111 [0/10352] [0/20676] 112 [0/10352] [0/20676] 113 [0/10352] [0/20676] 114 [0/10352] [0/20676] 115 [0/10352] [0/20676] 116 [0/10352] [0/20676] 117 [0/10352] [0/20676] 118 [0/10352] [0/20676] 119 [0/10352] [0/20676] 120 [0/10352] [0/20676] 121 [0/10352] [0/20676] 122 [0/10352] [0/20676] 123 [0/10352] [0/20676] 124 [0/10352] [0/20676] 125 [0/10352] [0/20676] 126 [0/10352] [0/20676] 127 [0/10352] [0/20676] 128 [0/10352] [0/20676] 129 [0/10352] [0/20676] 130 [0/10352] [0/20676] 131 [0/10352] [0/20676] 132 [0/10352] [0/20676] 133 [0/10352] [0/20676] 134 [0/10352] [0/20676] 135 [0/10352] [0/20676] 136 [0/10352] [0/20676] 137 [0/10352] [0/20676] 138 [0/10352] [0/20676] 139 [0/10352] [0/20676] 140 [0/10352] [0/20676] 141 [0/10352] [0/20676] 142 [0/10352] [0/20676] 143 [0/10352] [0/20676] 144 [0/10352] [0/20676] 145 [0/10352] [0/20676] 146 [0/10352] [0/20676] 147 [0/10352] [0/20676] 148 [0/10352] [0/20676] 149 [0/10352] [0/20676] 150 [0/10352] [0/20676] 151 [0/10352] [0/20676] 152 [0/10352] [0/20676] 153 [0/10352] [0/20676] 154 [0/10352] [0/20676] 155 [0/10352] [0/20676] 156 [0/10352] [0/20676] 157 [0/10352] [0/20676] 158 [0/10352] [0/20676] 159 [0/10352] [0/20676] 160 [0/10352] [0/20676] 161 [0/10352] [0/20676] 162 [0/10352] [0/20676] 163 [0/10352] [0/20676] 164 [0/10352] [0/20676] 165 [0/10352] [0/20676] 166 [0/10352] [0/20676] 167 [0/10352] [0/20676] 168 [0/10352] [0/20676] 169 [0/10352] [0/20676] 170 [0/10352] [0/20676] 171 [0/10352] [0/20676] 172 [0/10352] [0/20676] 173 [0/10352] [0/20676] 174 [0/10352] [0/20676] 175 [0/10352] [0/20676] 176 [0/10352] [0/20676] 177 [0/10352] [0/20676] 178 [0/10352] [0/20676] 179 [0/10352] [0/20676] 180 [0/10352] [0/20676] 181 [0/10352] [0/20676] 182 [0/10352] [0/20676] 183 [0/10352] [0/20676] 184 [0/10352] [0/20676] 185 [0/10352] [0/20676] 186 [0/10352] [0/20676] 187 [0/10352] [0/20676] 188 [0/10352] [0/20676] 189 [0/10352] [0/20676] 190 [0/10352] [0/20676] 191 [0/10352] [0/20676] 192 [0/10352] [0/20676] 193 [0/10352] [0/20676] 194 [0/10352] [0/20676] 195 [0/10352] [0/20676] 196 [0/10352] [0/20676] 197 [0/10352] [0/20676] 198 [0/10352] [0/20676] 199 [0/10352] [0/20676] 200 [0/10352] [0/20676] 201 [0/10352] [0/20676] 202 [0/10352] [0/20676] 203 [0/10352] [0/20676] 204 [0/10352] [0/20676] 205 [0/10352] [0/20676] 206 [0/10352] [0/20676] 207 [0/10352] [0/20676] 208 [0/10352] [0/20676] 209 [0/10352] [0/20676] 210 [0/10352] [0/20676] 211 [0/10352] [0/20676] 212 [0/10352] [0/20676] 213 [0/10352] [0/20676] 214 [0/10352] [0/20676] 215 [0/10352] [0/20676] 216 [0/10352] [0/20676] 217 [0/10352] [0/20676] 218 [0/10352] [0/20676] 219 [0/10352] [0/20676] 220 [0/10352] [0/20676] 221 [0/10352] [0/20676] 222 [0/10352] [0/20676] 223 [0/10352] [0/20676] 224 [0/10352] [0/20676] 225 [0/10352] [0/20676] 226 [0/10352] [0/20676] 227 [0/10352] [0/20676] 228 [0/10352] [0/20676] 229 [0/10352] [0/20676] 230 [0/10352] [0/20676] 231 [0/10352] [0/20676] 232 [0/10352] [0/20676] 233 [0/10352] [0/20676] 234 [0/10352] [0/20676] 235 [0/10352] [0/20676] 236 [0/10352] [0/20676] 237 [0/10352] [0/20676] 238 [0/10352] [0/20676] 239 [0/10352] [0/20676] 240 [0/10352] [0/20676] 241 [0/10352] [0/20676] 242 [0/10352] [0/20676] 243 [0/10352] [0/20676] 244 [0/10352] [0/20676] 245 [0/10352] [0/20676] 246 [0/10352] [0/20676] 247 [0/10352] [0/20676] 248 [0/10352] [0/20676] 249 [0/10352] [0/20676] 250 [0/10352] [0/20676] 251 [0/10352] [0/20676] 252 [0/10352] [0/20676] 253 [0/10352] [0/20676] 254 [0/10352] [0/20676] 255 [0/10352] [0/20676] 256 [0/10352] [0/20676] 257 [0/10352] [0/20676] 258 [0/10352] [0/20676] 259 [0/10352] [0/20676] 260 [0/10352] [0/20676] 261 [0/10352] [0/20676] 262 [0/10352] [0/20676] 263 [0/10352] [0/20676] 264 [0/10352] [0/20676] 265 [0/10352] [0/20676] 266 [0/10352] [0/20676] 267 [0/10352] [0/20676] 268 [0/10352] [0/20676] 269 [0/10352] [0/20676] 270 [0/10352] [0/20676] 271 [0/10352] [0/20676] 272 [0/10352] [0/20676] 273 [0/10352] [0/20676] 274 [0/10352] [0/20676] 275 [0/10352] [0/20676] 276 [0/10352] [0/20676] 277 [0/10352] [0/20676] 278 [0/10352] [0/20676] 279 [0/10352] [0/20676] 280 [0/10352] [0/20676] 281 [0/10352] [0/20676] 282 [0/10352] [0/20676] 283 [0/10352] [0/20676] 284 [0/10352] [0/20676] 285 [0/10352] [0/20676] 286 [0/10352] [0/20676] 287 [0/10352] [0/20676] 288 [0/10352] [0/20676] 289 [0/10352] [0/20676] 290 [0/10352] [0/20676] 291 [0/10352] [0/20676] 292 [0/10352] [0/20676] 293 [0/10352] [0/20676] 294 [0/10352] [0/20676] 295 [0/10352] [0/20676] 296 [0/10352] [0/20676] 297 [0/10352] [0/20676] 298 [0/10352] [0/20676] 299 [0/10352] [0/20676] 300 [0/10352] [0/20676] 301 [0/10352] 10352\n"
     ]
    }
   ],
   "source": [
    "X = X_train.copy()\n",
    "y = y_train.copy()\n",
    "\n",
    "X_test = X_sub.copy()\n",
    "test_y = np.array(([0.0] * len(X_test)))\n",
    "all_data_mask = np.array([False] * len(X_test))\n",
    "real_y = y_sub.copy()\n",
    "\n",
    "# 不均衡数据进行layer\n",
    "X_train_np = X\n",
    "y_train_np = y\n",
    "maxlayer = 300\n",
    "layer = 0\n",
    "\n",
    "enhancedDTree = EnhancedForest.EnhancedForest()\n",
    "counter = 0\n",
    "while 1:\n",
    "    layer += 1\n",
    "#     print()\n",
    "#     print(\"layer:\", layer)\n",
    "    X = X_train_np\n",
    "    y = y_train_np\n",
    "    clf, data_mask, all_false_data_index, p_test  = \\\n",
    "        enhancedDTree.TrainModelLayer(X, y, X_test, all_data_mask, test_y, real_y, verbose=False, feval=gini_metrix)\n",
    "    X_train_np = enhancedDTree.X_train_np\n",
    "    y_train_np = enhancedDTree.y_train_np\n",
    "    \n",
    "    # 打印信息\n",
    "    pass_data_id = data_mask[data_mask==True]\n",
    "    all_false_data_index = np.where(all_data_mask == False)[0]\n",
    "    X_test_np = X_test[all_false_data_index]\n",
    "    print(\"%d [%d/%d] \" % (layer, len(pass_data_id), len(X_test_np) - len(pass_data_id)), end=\"\")\n",
    "    \n",
    "    if X_train_np.shape[0] < 1000 or layer > maxlayer or y_train_np[y_train_np==1].shape[0] <= 10:\n",
    "        all_data_mask = all_false_data_index[~data_mask]\n",
    "        test_y[all_data_mask] = p_test[~data_mask]\n",
    "        print(len(p_test[~data_mask]))\n",
    "        break\n",
    "        \n",
    "    all_data_mask[~all_data_mask] = data_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Todo list\n",
    "- 树结构设计（完成）\n",
    "- 通过gini对数据分割（完成）\n",
    "- 全局测试集\n",
    "- 输出结果集\n",
    "- 打印信息增加pass data的比例\n",
    "- 防止过拟合\n",
    "- 对pass data的进一步处理\n",
    "- 先进行数据均衡化是不是更快一些"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
